# Cloud-Based Big Data Analytics using Apache Spark & Hadoop (Google Cloud Dataproc)

This project implements end-to-end Big Data Analytics using **Google Cloud Dataproc**, **Hadoop MapReduce**, **Apache Spark**, and **Spark MLlib** on the **Amazon Reviews 2023 dataset**.  
It is developed as part of the MSc Data Analytics coursework.

---

## ğŸ“Œ Project Overview

The objective of this project is to solve an e-commerce business problemâ€”customer sentiment analysis and trend identificationâ€”using Big Data tools and cloud technologies.

This repository contains:

- Google Cloud Dataproc setup documentation  
- Hadoop MapReduce data preprocessing  
- Apache Spark data engineering & analytics  
- Logistic Regression ML model using Spark MLlib  
- Performance comparison of MapReduce vs Spark  
- Visualizations & insights  

---

## ğŸš€ Technologies Used

- Google Cloud Platform  
  - Dataproc  
  - Cloud Storage (GCS)  
- Hadoop MapReduce  
- Apache Spark (PySpark)  
- Spark MLlib  
- Python 3.x  

---

## ğŸ“ Dataset â€“ Amazon Reviews 2023

- Size: **>10GB**  
- Source: Public GitHub Dataset  
- Contains:  
  - Product ID  
  - Customer ID  
  - Review Text  
  - Rating  
  - Timestamp  
  - Verified Purchase  
  - Sentiment  

âš ï¸ Due to size, the dataset is **not included** in this repository.  
Upload it to **Google Cloud Storage (GCS)** and access it from there.

---

## âš™ï¸ Cloud Architecture

<img width="1536" height="1024" alt="cloud Architecture" src="https://github.com/user-attachments/assets/04ff5408-925c-4758-8541-64d9fa93112c" />

---

## ğŸ”§ Setup & Execution

### 1ï¸âƒ£ Create and Configure Dataproc Cluster  
- Enable APIs  
- Create bucket  
- Configure master and worker nodes  
- Choose appropriate machine types


### 2ï¸âƒ£ Upload Dataset to Cloud Storage

gsutil cp dataset.json gs://your-bucket/raw/

### 3ï¸âƒ£ Copy Data to HDFS

hdfs dfs -mkdir /data
hdfs dfs -copyFromLocal dataset.json /data/

---

## ğŸ—‚ Hadoop MapReduce Job

Example run:

hadoop jar wordcount.jar /data/dataset.json /output/wordcount 

Code available in:

src/mapreduce/


---
## ğŸ”¥ Spark Analysis Job

Submit PySpark job:

gcloud dataproc jobs submit pyspark src/spark/spark_analysis.py --cluster=yourcluster

Outputs stored in HDFS.

---

## ğŸ¤– Machine Learning â€“ Logistic Regression (Spark MLlib)

Used for binary sentiment classification

Accuracy: 87%

Metrics used: Precision, Recall, F1-Score

Balanced on 80/20 train-test split

Notebook:

notebooks/model_training.ipynb

---

## ğŸ“Š Visualizations

Place generated images in:

images/

Examples:

Positive vs negative sentiment distribution

Rating trends

Verified vs non-verified purchase comparison

---

## ğŸ“Œ Key Insights

Negative reviews help identify product weaknesses

Sentiment data improves product recommendation systems

Helps in inventory and supply chain optimization

Detects unusual/fake reviews

Real-time insights enhance customer satisfaction

---

## ğŸ‘©â€ğŸ’» Author

Bhavikaben Radadiya
MSc Data Analytics â€“ 2025
GitHub:https://github.com/bhavikaradadiya

## ğŸ“œ License

This project is licensed under the MIT License.
See the LICENSE file for details.
